{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "[Chainer](http://chainer.org/) とはニューラルネットの実装を簡単にしたフレームワークです。\n",
    "\n",
    "* 今回は対話にニューラルネットを適用してみました。\n",
    "\n",
    "![](./pictures/Chainer.jpg)\n",
    "\n",
    "* 今回は対話モデルの構築を行っていただきます。\n",
    "\n",
    "\n",
    "対話モデルは人間の発言に対して応答して返すモデルです。\n",
    "\n",
    "対話モデルにはいくつか種類があるのでここでも紹介しておきます。\n",
    "\n",
    "* POMDP\n",
    " * [POMDP](https://github.com/pybrain/pybrain/blob/master/pybrain/rl/environments/mazes/tasks/pomdp.py)対話モデルの一般的な機械学習のモデルです。\n",
    "* End TO End ニューラル対話モデル\n",
    " * 入力をユーザーの入力、出力をシステム側の出力として機械翻訳と同じ枠組みで対応しているモデル\n",
    "\n",
    "以下では、このChainerを利用しデータを準備するところから実際にNN対話モデルを構築し学習・評価を行うまでの手順を解説します。\n",
    "\n",
    "<A HREF=#1.各種ライブラリ導入 >1.各種ライブラリ導入</A><br>\n",
    "<A HREF=#2.対話のクラス >2.対話のクラス</A><br>\n",
    "<A HREF=#3.各値を設定 >3.各値を設定</A><br>\n",
    "<A HREF=#4.実行 >4.実行</A><br>\n",
    "<A HREF=#5.学習したモデルの動作テスト >5.学習したモデルの動作テスト</A><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=1.各種ライブラリ導入 /> 1.各種ライブラリ導入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chainerの言語処理では多数のライブラリを導入します。\n",
    "Ctrl → m → lをコードの部分で入力すると行番号が出ます。ハンズオンの都合上、行番号があった方が良いので対応よろしくお願いします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# unit: 300\n",
      "Window : 5\n",
      "Minibatch-size: 100\n",
      "# epoch: 10\n",
      "Traing model: skipgram\n",
      "Output type: original\n"
     ]
    }
   ],
   "source": [
    "#表示用に使用しています。\n",
    "from util.functions import trace\n",
    "import numpy as np\n",
    "\n",
    "from chainer import Chain, Variable, cuda, functions, links, optimizer, optimizers, serializers\n",
    "import chainer.links as L\n",
    "\n",
    "from EncoderDecoderModel import EncoderDecoderModel\n",
    "import subprocess\n",
    "\n",
    "from word2vec.word2vec_load import SkipGram,SoftmaxCrossEntropyLoss\n",
    "\n",
    "unit = 300\n",
    "vocab = 5000\n",
    "loss_func = SoftmaxCrossEntropyLoss(unit, vocab)\n",
    "w2v_model = SkipGram(vocab, unit, loss_func)\n",
    "serializers.load_hdf5(\"word2vec/word2vec_chainer.model\", w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`導入するライブラリの代表例は下記です。\n",
    "\n",
    "* `numpy`: 行列計算などの複雑な計算を行なうライブラリ\n",
    "* `chainer`: Chainerの導入\n",
    "* `util`:今回の処理で必要なライブラリが入っています。\n",
    "* `w2v_model`:初期値の設定にWord2vecを使用して初期値の最適化を行なっています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=2.対話のクラス /> 2.対話のクラス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下記の論文を参考にしてforward処理を記述しています。\n",
    "\n",
    "http://arxiv.org/pdf/1507.04808.pdf\n",
    "\n",
    "\n",
    "下記を設定しています。\n",
    "* ニューラルネットを用いて対話用のモデルを構成しています。\n",
    "\n",
    "全体構成\n",
    "\n",
    "![](./pictures/NN_machine_translation.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderModelForward(EncoderDecoderModel):\n",
    "    \n",
    "    def forward(self, src_batch, trg_batch, src_vocab, trg_vocab, encdec, is_training, generation_limit):\n",
    "        batch_size = len(src_batch)\n",
    "        src_len = len(src_batch[0])\n",
    "        trg_len = len(trg_batch[0]) if trg_batch else 0\n",
    "        src_stoi = src_vocab.stoi\n",
    "        trg_stoi = trg_vocab.stoi\n",
    "        trg_itos = trg_vocab.itos\n",
    "        encdec.reset(batch_size)\n",
    "\n",
    "        x = self.common_function.my_array([src_stoi('</s>') for _ in range(batch_size)], np.int32)\n",
    "        encdec.encode(x)\n",
    "        for l in reversed(range(src_len)):\n",
    "            x = self.common_function.my_array([src_stoi(src_batch[k][l]) for k in range(batch_size)], np.int32)\n",
    "            encdec.encode(x)\n",
    "\n",
    "        t = self.common_function.my_array([trg_stoi('<s>') for _ in range(batch_size)], np.int32)\n",
    "        hyp_batch = [[] for _ in range(batch_size)]\n",
    "\n",
    "        if is_training:\n",
    "            loss = self.common_function.my_zeros((), np.float32)\n",
    "            for l in range(trg_len):\n",
    "                y = encdec.decode(t)\n",
    "                t = self.common_function.my_array([trg_stoi(trg_batch[k][l]) for k in range(batch_size)], np.int32)\n",
    "                loss += functions.softmax_cross_entropy(y, t)\n",
    "                output = cuda.to_cpu(y.data.argmax(1))\n",
    "                for k in range(batch_size):\n",
    "                    hyp_batch[k].append(trg_itos(output[k]))\n",
    "            return hyp_batch, loss\n",
    "\n",
    "        else:\n",
    "            while len(hyp_batch[0]) < generation_limit:\n",
    "                y = encdec.decode(t)\n",
    "                output = cuda.to_cpu(y.data.argmax(1))\n",
    "                t = self.common_function.my_array(output, np.int32)\n",
    "                for k in range(batch_size):\n",
    "                    hyp_batch[k].append(trg_itos(output[k]))\n",
    "                if all(hyp_batch[k][-1] == '</s>' for k in range(batch_size)):\n",
    "                    break\n",
    "\n",
    "        return hyp_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=3.各値を設定 /> 3.各値を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各値を設定\n",
    "\n",
    "* ユーザーの発言の設定(学習データ)\n",
    "* システム応答結果の設定（学習データ）\n",
    "* ユーザーの発言の設定(テストデータ)\n",
    "* システム応答結果の設定（テストデータ）\n",
    "* 語彙の設定\n",
    "* 潜在空間の設定\n",
    "* 隠れ層の設定\n",
    "* 学習回数の設定\n",
    "* ミニバッチサイズの設定\n",
    "* 最大予測言語数の設定\n",
    "ベストな調整方法は経験則か力技です。グリッドサーチ、ランダムサーチ、データから推定など。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameter_dict = {}\n",
    "train_path = \"data/\"\n",
    "parameter_dict[\"source\"] = train_path + \"player_1_wakati\"\n",
    "parameter_dict[\"target\"] = train_path + \"player_2_wakati\"\n",
    "parameter_dict[\"test_source\"] = train_path + \"player_1_wakati\"\n",
    "parameter_dict[\"test_target\"] = train_path + \"player_2_test\"\n",
    "#--------Hands on  2----------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "下記の値が大きいほど扱える語彙の数が増えて表現力が上がるが計算量が爆発的に増えるので大きくしない方が良いです。\n",
    "\"\"\"\n",
    "parameter_dict[\"vocab\"] = 5000\n",
    "\n",
    "\"\"\"\n",
    "この数が多くなればなるほどモデルが複雑になります。この数を多くすると必然的に学習回数を多くしないと学習は\n",
    "収束しません。\n",
    "語彙数よりユニット数の数が多いと潜在空間への写像が出来ていないことになり結果的に意味がない処理になります。\n",
    "\"\"\"\n",
    "parameter_dict[\"embed\"] = 300\n",
    "\n",
    "\"\"\"\n",
    "この数も多くなればなるほどモデルが複雑になります。この数を多くすると必然的に学習回数を多くしないと学習は\n",
    "収束しません。\n",
    "\"\"\"\n",
    "parameter_dict[\"hidden\"] = 500\n",
    "\n",
    "\"\"\"\n",
    "学習回数。基本的に大きい方が良いが大きすぎると収束しないです。\n",
    "\"\"\"\n",
    "parameter_dict[\"epoch\"] = 20\n",
    "\n",
    "\"\"\"\n",
    "ミニバッチ学習で扱うサイズです。この点は経験的に調整する場合が多いが、基本的に大きくすると学習精度が向上する\n",
    "代わりに学習スピードが落ち、小さくすると学習精度が低下する代わりに学習スピードが早くなります。\n",
    "\"\"\"\n",
    "parameter_dict[\"minibatch\"] = 64\n",
    "\n",
    "\"\"\"\n",
    "予測の際に必要な単語数の設定。長いほど多くの単語の翻訳が確認できるが、一般的にニューラル翻訳は長い翻訳には\n",
    "向いていないので小さい数値がオススメです。\n",
    "\"\"\"\n",
    "parameter_dict[\"generation_limit\"] = 256\n",
    "\n",
    "parameter_dict[\"word2vec\"] = w2v_model\n",
    "\n",
    "parameter_dict[\"word2vecFlag\"] = True\n",
    "\n",
    "\n",
    "parameter_dict[\"encdec\"] = \"\"\n",
    "\n",
    "#--------Hands on  2----------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=4.実行 /> 4.実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-01-04 11:35:40.057064 ... initializing ...\n",
      "2016-01-04 11:35:40.057769 ... making vocabularies ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/player_1_wakati'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d2c134c8f13e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mencoderDecoderModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderDecoderModelForward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameter_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoderDecoderModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/smap2/Research/Chainer-Slack-Twitter-Dialogue/EncoderDecoderModel.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'making vocabularies ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0msrc_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mtrg_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/smap2/Research/Chainer-Slack-Twitter-Dialogue/util/vocabulary.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(list_generator, size)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mword_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_generator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mword_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/smap2/Research/Chainer-Slack-Twitter-Dialogue/util/generators.py\u001b[0m in \u001b[0;36mword_list\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mword_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/player_1_wakati'"
     ]
    }
   ],
   "source": [
    "trace('initializing ...')\n",
    "\n",
    "encoderDecoderModel = EncoderDecoderModelForward(parameter_dict)\n",
    "encoderDecoderModel.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <A NAME=5.学習したモデルの動作テスト /> 5.学習したモデルの動作テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習したモデルを使用してテスト\n",
    "\n",
    "* 学習したモデルを利用して学習データに対して対応を変えす。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"ChainerDialogue.021\"\n",
    "trace('initializing ...')\n",
    "\n",
    "encoderDecoderModel = EncoderDecoderModelForward(parameter_dict)\n",
    "encoderDecoderModel.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
